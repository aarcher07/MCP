{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEMS 395-1 HW1(template for your own solution)\n",
    "\n",
    "# Newton's method with line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019, Andreas Waechter\n",
    "\n",
    "Distribution of this file is not allowed without explicit permission of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make NumPy module available for numerical computations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize options for Newton's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeObjInitOptions():\n",
    "    '''\n",
    "    Option initialization for minimize_obj function\n",
    "\n",
    "    Initialize algorithm options with default values\n",
    "\n",
    "    Return values:\n",
    "        options:\n",
    "            This is a dictionary with fields that correspond to algorithmic\n",
    "            options of our method.  In particular:\n",
    "\n",
    "            max_iter:\n",
    "                Maximum number of iterations\n",
    "            tol:\n",
    "                Convergence tolerance\n",
    "            step_type:\n",
    "                Different ways to calculate the search direction:\n",
    "               'Steepest Descent'\n",
    "               'Newton'\n",
    "            init_alpha:\n",
    "                first trial step size to attempt during line search\n",
    "            suff_decrease_c1:\n",
    "                coefficient in sufficient decrease condition\n",
    "            output_level:\n",
    "                Amount of output printed\n",
    "                0: No output\n",
    "                1: Only summary information\n",
    "                2: One line per iteration (good for debugging)\n",
    "    '''\n",
    "    # we first need to create an empty dictionary before we can add entries\n",
    "    options = {}\n",
    "\n",
    "    # Now set a default value for each of the options\n",
    "    options['max_iter'] = 1e6\n",
    "    options['tol'] = 1e-6\n",
    "    options['step_type'] = 'Newton'\n",
    "    options['init_alpha'] = 1.\n",
    "    options['suff_decrease_c1'] = 1e-4\n",
    "    options['output_level'] = 2\n",
    "\n",
    "    return options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosenbrock:\n",
    "    '''\n",
    "    Implementation of the Rosenbrock objective function\n",
    "\n",
    "    a*(x[1] - x[0]**2)**2 + (b - x[0])**2\n",
    "\n",
    "    Parameters:\n",
    "        a: scalar, see formula\n",
    "        b: scalar, see formula\n",
    "    '''\n",
    "\n",
    "    def __init__(self, a=100.0, b=1.0):\n",
    "        '''\n",
    "        Initialize object with parameters a and b in the formula.\n",
    "        '''\n",
    "        self._a = a\n",
    "        self._b = b\n",
    "\n",
    "    def value(self, x):\n",
    "        '''\n",
    "        Compute value of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        val = a*(x[1] - x[0]**2)**2 + (b - x[0])**2\n",
    "        return val\n",
    "\n",
    "    def gradient(self, x):\n",
    "        '''\n",
    "        Compute gradient of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        grad = np.array([0, 0])\n",
    "        return grad\n",
    "\n",
    "    def hessian(self, x):\n",
    "        '''\n",
    "        Compute Hessian of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        Hess = 0\n",
    "        return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeObjective(x_start, objFunc, options):\n",
    "    '''\n",
    "    Optimization method for unconstrained optimization\n",
    "\n",
    "    This is the template for your solution.  You need to add or change\n",
    "    things, not only at the places pointed out explicitly\n",
    "\n",
    "    Input arguments:\n",
    "        x_start:\n",
    "            Starting point\n",
    "        objFunc:\n",
    "            Objective function object.  It must have the following methods,\n",
    "            where x is the point where the quantity should be evaluated.\n",
    "\n",
    "            val = value(x)\n",
    "                returns value of objective function at x\n",
    "            grad = gradient(x)\n",
    "                returns gradient of objective function at x\n",
    "            Hess = hessian(x)\n",
    "                return Hessian of objective function at x\n",
    "        options:\n",
    "            This is a dictionary with options for the algorithm.\n",
    "            For details see the minimizeObjInitOptions function.\n",
    "\n",
    "    Return values:\n",
    "        status:\n",
    "           Return code indicating reason for termination:\n",
    "            0:  Optimal solution found (convergence tolerance satisfied)\n",
    "           -1:  Maximum number of iterations exceeded\n",
    "           -2:  Search direction is not a descent direction\n",
    "          -99:  Unknown error (bug?)\n",
    "        x_sol:\n",
    "            Approximate critical point (or last iterate if there is a failure)\n",
    "        f_sol:\n",
    "            objective function value at x_sol\n",
    "        stats:\n",
    "            Dictionary with statistics for the run.  Its fields are\n",
    "            norm_grad      Norm of gradient at final iterate\n",
    "            num_iter       Number of iterations taken\n",
    "            num_func_evals Number of function evaluations needed\n",
    "    '''\n",
    "\n",
    "    # initialize iteration counter\n",
    "    iter_count = 0\n",
    "\n",
    "    # initialize return flag (set to -99 so that we do not accidentally\n",
    "    # declare success.)\n",
    "    status = -99\n",
    "    \n",
    "    # get parameters out put the options dictionary\n",
    "    max_iter = options['max_iter']\n",
    "    tol = options['tol']\n",
    "    step_type = options['step_type']\n",
    "    init_alpha = options['init_alpha']\n",
    "    output_level = options['output_level']\n",
    "\n",
    "    # initialize current iterate\n",
    "    #\n",
    "    # ****Important note:****\n",
    "    # we need to make a copy of the x_start array.  If we would\n",
    "    # just write \"x_k = x_start\", then the Python variables x_k and x_start\n",
    "    # would be different names for the same array, and then changing\n",
    "    # x_k would also change x_start.  Since we do not want to change what is\n",
    "    # stored as the starting point, we need to make a copy here.\n",
    "    x_k = np.copy(x_start)\n",
    "\n",
    "    # initialize current function value, gradient, and infinity norm of the\n",
    "    # gradient\n",
    "    f_k = objFunc.value(x_k)\n",
    "    grad_k = objFunc.gradient(x_k)\n",
    "    norm_grad_k = np.linalg.norm(grad_k, np.inf)\n",
    "\n",
    "    # initialize counter for function evaluations\n",
    "    num_func_evals = 1\n",
    "\n",
    "    # determine how many variables are in the problem\n",
    "    num_vars = len(x_start)\n",
    "\n",
    "    # initialize search direction and its length to zero\n",
    "    p_k = np.zeros(num_vars)\n",
    "    norm_pk = 0.0\n",
    "\n",
    "    # initialize step size to zero (this is only necessary so that we can print\n",
    "    # an output line before a meaningful step size has actually been computed.)\n",
    "    alpha_k = 0.0\n",
    "\n",
    "    # initialize the counter for the function evaluations needed in a\n",
    "    # particular iteration\n",
    "    num_func_it = 0\n",
    "\n",
    "    # Print header and zero-th iteration for output\n",
    "    if output_level >= 2:\n",
    "        # (This is just a fancy way to create a string.  The '%s' formatting\n",
    "        # makes it easy to align the header with the actual output.)\n",
    "        output_header = '%6s %23s %9s %9s %6s %9s' % \\\n",
    "            ('iter', 'f', '||p_k||', 'alpha', '#func', '||grad_f||')\n",
    "        print(output_header)\n",
    "        print('%6i %23.16e %9.2e %9.2e %6i %9.2e' %\n",
    "              (iter_count, f_k, norm_pk, alpha_k, num_func_it, norm_grad_k))\n",
    "\n",
    "    ###########################\n",
    "    # Beginning of main Loop\n",
    "    ###########################\n",
    "\n",
    "    # We continue the main loop until the termination tolerance is met.\n",
    "    while 1:\n",
    "\n",
    "        ##############################################################\n",
    "        # Check termination tests\n",
    "        ##############################################################\n",
    "        if 1. <= tol:\n",
    "            # Termination tolerance met\n",
    "            status = 0\n",
    "            break\n",
    "\n",
    "        if iter_count >= 100:\n",
    "            # Set flag to indicate the maximum number of iterations has been\n",
    "            # exceeded\n",
    "            status = -1\n",
    "            # The following command says that we now want to leave the current\n",
    "            # loop (which is the while loop here).  The program execution will\n",
    "            # resume immediately after the end of the loop\n",
    "            break\n",
    "\n",
    "        ##############################################################\n",
    "        # Compute search direction\n",
    "        ##############################################################\n",
    "        if step_type == 'Newton':\n",
    "            p_k = np.zeros(num_vars)  # Obviously, put something reasonable here\n",
    "        elif step_type == 'gradient_descent':\n",
    "            p_k = np.zeros(num_vars)  # Obviously, put something reasonable here\n",
    "        else:\n",
    "            raise ValueError('Invalid value for options[step_type]')\n",
    "\n",
    "        ##############################################################\n",
    "        # Perform the backtracking Armijo line search\n",
    "        ##############################################################\n",
    "\n",
    "        # initialize step size\n",
    "        alpha_k = init_alpha\n",
    "\n",
    "        # Compute trial point and objective value at trial point\n",
    "        x_trial = x_k + alpha_k*p_k\n",
    "        f_trial = objFunc.value(x_trial)\n",
    "        num_func_it = 1\n",
    "        \n",
    "        # Put your code here\n",
    "\n",
    "        # Update iterate\n",
    "        x_k = np.copy(x_trial)\n",
    "        f_k = f_trial\n",
    "\n",
    "        # Compute gradient and its norm at the new iterate\n",
    "        grad_k = objFunc.gradient(x_k)\n",
    "        norm_grad_k = np.linalg.norm(grad_k, np.inf)\n",
    "\n",
    "        # For the output, compute the norm of the step\n",
    "        norm_pk = np.linalg.norm(p_k, np.inf)\n",
    "\n",
    "        # Update counter for total number of function evaluations\n",
    "        num_func_evals += num_func_it\n",
    "\n",
    "        # Increase the iteration counter\n",
    "        iter_count += 1\n",
    "\n",
    "        # Iteration output\n",
    "        if output_level >= 2:\n",
    "            # Print the output header every 10 iterations\n",
    "            if iter_count % 10 == 0:\n",
    "                print(output_header)\n",
    "            print('%6i %23.16e %9.2e %9.2e %6i %9.2e' %\n",
    "                  (iter_count, f_k, norm_pk, alpha_k, num_func_it, norm_grad_k))\n",
    "\n",
    "    ###########################\n",
    "    # End of main loop\n",
    "    ###########################\n",
    "\n",
    "    ###########################\n",
    "    # Finalize results\n",
    "    ###########################\n",
    "\n",
    "    # Set last iterate as the one that is returned, together with its objective\n",
    "    # value\n",
    "    x_sol = x_k\n",
    "    f_sol = f_k\n",
    "\n",
    "    # Set the statistics\n",
    "    stats = {}\n",
    "    stats['num_iter'] = iter_count\n",
    "    stats['norm_grad'] = norm_grad_k\n",
    "    stats['num_func_evals'] = num_func_evals\n",
    "\n",
    "    # Final output message\n",
    "    if output_level >= 1:\n",
    "        print('')\n",
    "        print('Final objective.................: %g' % f_sol)\n",
    "        print('||grad|| at final point.........: %g' % norm_grad_k)\n",
    "        print('Number of iterations............: %d' % iter_count)\n",
    "        print('Number of function evaluations..: %d' % num_func_evals)\n",
    "        print('')\n",
    "        if status == 0:\n",
    "            print('Exit: Critical point found.')\n",
    "        elif status == -1:\n",
    "            print('Exit: Maximum number of iterations (%d) exceeded.' %\n",
    "                  iter_count)\n",
    "        elif status == -2:\n",
    "            print('Exit: Search direction is not a descent direction.')\n",
    "        elif status == -3:\n",
    "            print('Exit: Exit: Step size becomes zero.')\n",
    "        else:\n",
    "            print('ERROR: Unknown status value: %d\\n' % status)\n",
    "\n",
    "    # Return output arguments\n",
    "    return status, x_sol, f_sol, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Rosenbrock with steepest descent from first starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "x_start = np.array([1.2, 1.2])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective( x_start, objFunc, options )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
